from airflow.providers.amazon.aws.hooks.s3 import S3Hook

class Loader:
    def __init__(self, university, logger=None) -> None:
        self.university = university
        self.logger = logger

        self.S3_ID = "aws_s3_bucket"
        self.dest_bucket = 'alkemy26'
        self.path = './datasets/'
        self.key = f'{self.university}_process.txt'
        
    def loading(self):
        s3_hook = S3Hook(aws_conn_id=self.S3_ID)

        s3_hook.load_file(
            self.path + self.key,
            key=self.key,
            bucket_name=self.dest_bucket,
            replace=True,
            encrypt=False,
            gzip=False,
        )

    def to_load(self):
        if self.logger:
            self.logger.info('Iniciando proceso de cargue de datos al bucket de S3')

        try:
            self.loading()

            if self.logger:
                self.logger.info('Tarea de carga EXITOSA')

        except Exception as e:
            if self.logger:
                self.logger.info('ERROR al cargar los datos al bucket de S3')
                self.logger.error(e)
